{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install pandas-profiling","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U dataprep","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom pandas import Series\nimport pandas_profiling\nfrom pandas_profiling import ProfileReport\nimport warnings\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n%matplotlib inline\nfrom sklearn.feature_selection import f_classif, mutual_info_classif\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import auc, roc_auc_score, roc_curve\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\n#import dataprep.eda\nfrom datetime import datetime\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model(): # класс для оценки метрик модели\n    def __init__(self, model, X_test, y_test):\n        self.X_test = X_test\n        self.y_test = y_test\n        self.y_pred = model.predict(X_test)\n        self.probs = model.predict_proba(X_test)[:,1]\n    \n    def roc_curve(self):\n        fpr, tpr, threshold = roc_curve(self.y_test, self.probs)\n        roc_auc = roc_auc_score(self.y_test, self.probs)\n\n        plt.figure()\n        plt.plot([0, 1], label='Baseline', linestyle='--')\n        plt.plot(fpr, tpr, label = 'Regression')\n        plt.title('Logistic Regression ROC AUC = %0.05f' % roc_auc)\n        plt.ylabel('True Positive Rate')\n        plt.xlabel('False Positive Rate')\n        plt.legend(loc = 'lower right')\n        plt.show()\n    \n    def confusion_matrix(self):\n        tn, fp, fn, tp = confusion_matrix(self.y_test, self.y_pred).ravel()\n        cf_matrix = np.array([[tp,fp],[fn,tn]])\n        group_names = ['TP','FP','FN','TN']\n        group_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]\n        labels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names,group_counts)]\n        labels = np.asarray(labels).reshape(2,2)\n        plt.figure()\n        sns.heatmap(cf_matrix, annot=labels, annot_kws={\"size\": 20}, fmt='', cmap= 'Pastel1', cbar = False, \\\n                 xticklabels = ['Дефолт','Не дефолт'], yticklabels= ['Дефолт','Не дефолт'])\n        plt.title('Матрица ошибок')\n        plt.show()\n        \n    def get_metrics(self):\n        result = pd.Series({\n            'Accuracy' : accuracy_score(self.y_test, self.y_pred),\n            'Precision' : precision_score(self.y_test, self.y_pred),\n            'Recall' : recall_score(self.y_test, self.y_pred),\n            'F1' : f1_score(self.y_test, self.y_pred),\n            'ROC_AUC': roc_auc_score(self.y_test, self.probs) \n        })\n        return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RANDOM_SEED = 42\n!pip freeze > requirements.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_DIR = '/kaggle/input/sf-dst-scoring/'\ntrain = pd.read_csv(DATA_DIR+'/train.csv')\ntest = pd.read_csv(DATA_DIR+'test.csv')\nsample_submission = pd.read_csv(DATA_DIR+'/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Посмотрим на данные трейна и теста.","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# для корректной обработки признаков объединяем трейн и тест в один датасет\ntrain['Train'] = 1 # помечаем где у нас трейн\ntest['Train'] = 0 # помечаем где у нас тест\n\ndata = train.append(test, sort=False).reset_index(drop=True) # объединяем","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Информация о пропусках во всём датасете.\ndata.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"В итоге видим, что в тестовой выборке отсутствует колонка default, соответственно большое кол-во пропусков в общем датасете.","metadata":{}},{"cell_type":"code","source":"data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Описание датасета\n* client_id - идентификатор клиента\n* education - уровень образования\n* sex - пол заемщика\n* age - возраст заемщика\n* car - наличие автомобиля\n* car_type - наличие автомобиля иномарки\n* decline_app_cnt - количество отказанных прошлых заявок\n* good_work - наличие хорошей работы\n* bki_request_cnt - количество запросов в БКИ\n* home_address - категоризатор домашнего адреса\n* work_address - категоризатор рабочего адреса\n* income - доход заемщика\n* foreign_passport - наличие загранпаспорта\n* sna - связь заемщика с клиентами банка\n* first_time - давность наличия информации о заемщике\n* score_bki - скоринговый балл по данным из БКИ\n* region_rating - рейтинг региона\n* app_date - дата подачи заявки\n* default - дефолт по кредиту","metadata":{}},{"cell_type":"code","source":"# Кол-во дубликатов. (На всякий случай)\ndata.duplicated().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Распределение целевой переменной\ntrain['default'].value_counts().plot.bar()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Увы, но неравномерное.","metadata":{}},{"cell_type":"markdown","source":"## Далее рассмотрим подробнее наши признаки:","metadata":{}},{"cell_type":"code","source":"ProfileReport(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Зададим списки числовых, категориальных и бинарных признаков\nnum_cols = ['age','score_bki', 'bki_request_cnt','decline_app_cnt', 'income']\ncat_cols = ['home_address', 'work_address', 'sna', 'first_time', 'region_rating','education']\nbin_cols = ['sex', 'car', 'car_type', 'good_work', 'foreign_passport']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataprep.eda.plot(data, 'age')\n# Видим, что самый молодой заёмщик 21 год, а самый старый 72. Средний возраст 39 лет.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataprep.eda.plot(data, 'score_bki')\n# Видимо признак был ранее прологарифмирован. Трогать его не будем.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataprep.eda.plot(data, 'bki_request_cnt')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_bki = data.groupby('age')['bki_request_cnt'].count().to_dict()\ncount_bki\n# Количество обращений больше в 26-32 года.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataprep.eda.plot(data, 'decline_app_cnt')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataprep.eda.plot(data, 'income')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataprep.eda.plot(train, 'income', 'default')\n# Видим, что дефолтные заёмщики с невысоким доходом.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Теперь прологарифмируем признаки.\ndata['age'] = np.log(data['age'] + 1)\ndata['income'] = np.log(data['income'] + 1)\ndata['decline_app_cnt'] = np.log(data['decline_app_cnt'] + 1)\ndata['bki_request_cnt'] = np.log(data['bki_request_cnt'] + 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Для категориальных признаков будем делать get_dummies.","metadata":{}},{"cell_type":"code","source":"dataprep.eda.plot(data, 'education')\n# Мы видим, что со школьным образованием больше всего, почти 53%. \n# А людей с академическим образованием меньше всех, почти 0,3%. \n# Также видим тенденцию - чем лучше образование, тем меньше берут кредиты.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Пропусков не так много, поэтому заполним школьным образованием.\ndata['education'] = data['education'].fillna('SCH')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataprep.eda.plot(data, 'region_rating')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataprep.eda.plot(data, 'region_rating', 'default')\n# Чем ниже рейтинг региона, тем вероятнее невыплата кредита.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataprep.eda.plot(data, 'first_time')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataprep.eda.plot(data, 'sna')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataprep.eda.plot(data, 'home_address')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataprep.eda.plot(data, 'work_address')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataprep.eda.plot(data, 'sex')\n# Вывод простой - женщины берут кредиты чаще, чем мужчины, правда не намного: 56% против 44%.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataprep.eda.plot(data, 'car')\n# Треть имеет автомобиль","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataprep.eda.plot(data, 'car_type')\n# Три четверти заёмщиков ездит на отечественном автомобиле.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataprep.eda.plot(data, 'good_work')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataprep.eda.plot(data, 'foreign_passport')\n# У 15% есть загранпаспорт.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Закодируем бинарные переменные.\nlabel_encoder = LabelEncoder()\n\nfor column in bin_cols:\n    data[column] = label_encoder.fit_transform(data[column])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Переведём колонку в цифровой формат.\nedu_dict = {'SCH': 1,'UGR': 2,'GRD': 3,'PGR': 4,'ACD': 5}\ndata['education'] = data['education'].map(edu_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Преобразуем формат признака app_date\ndata.app_date = pd.to_datetime(data.app_date, format='%d%b%Y')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Начало и конец периода нашего датасета\nstart = data.app_date.min()\nend = data.app_date.max()\nprint(start, end)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Новые признаки\n\nmax_income = data.groupby('age')['income'].max().to_dict()\ndata['sign_1'] = data['age'].map(max_income)\n\nmean_income_region = data.groupby('region_rating')['income'].mean().to_dict()\ndata['sign_2'] = data['region_rating'].map(mean_income_region)\n\nmean_income_age = data.groupby('age')['income'].mean().to_dict()\ndata['sign_3'] = data['age'].map(mean_income_age)\n\nmean_bki = data.groupby('age')['bki_request_cnt'].mean().to_dict()\ndata['sign_4'] = data['age'].map(mean_bki)\n\ndata['sign_5'] = data.region_rating * data.car\ndata['sign_6'] = data.home_address + data.work_address\ndata['sign_7'] = data.age / data.score_bki\ndata['sign_8'] = (data.home_address + data.work_address) * data.good_work \ndata['sign_9'] = (data.home_address + data.work_address) * data.sna \ndata['sign_10'] = data.decline_app_cnt * data.sna\ndata['sign_11'] = data.income - data.age.map(mean_income_age)\ndata['sign_12'] = data.income/data.age\ndata['sign_13'] = data.education/data.age\n\nnew_cols = ['sign_1', 'sign_2', 'sign_3', 'sign_4', 'sign_5', 'sign_6', 'sign_7', 'sign_8', 'sign_9', 'sign_10',\n           'sign_11', 'sign_12', 'sign_13']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Удалим из датасета признак app_date, он нам больше не понадобится.\ndata.drop(['app_date'],  axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Нормализуем данные.\nnum_cols = ['age', 'decline_app_cnt', 'bki_request_cnt', 'income']\nscaler = StandardScaler()\nfor col in num_cols:\n    data[col] = scaler.fit_transform(data[[col]])\nfor col in new_cols:\n    data[col] = scaler.fit_transform(data[[col]]) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Преобразуем категориальные признаки.\ndata=pd.get_dummies(data, prefix=cat_cols, columns=cat_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Перейдём к модели.","metadata":{}},{"cell_type":"code","source":"# Разбиваем датасет на тренировочный и тестовый.\ntrain_data = data.query('Train == 1').drop(['Train', 'client_id'], axis=1)\ntest_data = data.query('Train == 0').drop(['Train', 'client_id'], axis=1)\n\ny = train_data.default.values\nX = train_data.drop(['default'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Подбираем параметры для модели LogisticRegression. Код закомментирован, т.к. выполняется за длительное время. ","metadata":{}},{"cell_type":"code","source":"#model = LogisticRegression(random_state=RANDOM_SEED)\n\n#iter_ = 100\n#epsilon_stop = 1e-3\n\n#param_grid = [\n#    {'penalty': ['l1'], \n#     'solver': ['liblinear', 'lbfgs'], \n#     'class_weight':['none', 'balanced'], \n#     'multi_class': ['auto','ovr'], \n#     'max_iter':[iter_],\n#     'tol':[epsilon_stop]},\n#    {'penalty': ['l2'], \n#     'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], \n#     'class_weight':['none', 'balanced'], \n#     'multi_class': ['auto','ovr'], \n#     'max_iter':[iter_],\n#     'tol':[epsilon_stop]},\n#    {'penalty': ['none'], \n#     'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'], \n#     'class_weight':['none', 'balanced'], \n#     'multi_class': ['auto','ovr'], \n#     'max_iter':[iter_],\n#     'tol':[epsilon_stop]},\n#]\n#gridsearch = GridSearchCV(model, param_grid, scoring='f1', n_jobs=-1, cv=5)\n#gridsearch.fit(X_train, y_train)\n#model = gridsearch.best_estimator_\n# Параметры\n#best_parameters = model.get_params()\n#for param_name in sorted(best_parameters.keys()):\n#        print('\\t%s: %r' % (param_name, best_parameters[param_name]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_model = LogisticRegression(random_state=42, \n                           C=1.0, \n                           class_weight= 'balanced', \n                           dual= False, \n                           fit_intercept= True, \n                           intercept_scaling= 1, \n                           l1_ratio= None,\n                           max_iter = 100,\n                           multi_class= 'auto', \n                           n_jobs= None,\n                           penalty = 'l2',\n                           solver = 'lbfgs',\n                           tol = 0.001,\n                           verbose= 0, \n                           warm_start= False)\n\nlog_model.fit(X_train, y_train)\n\ny_pred_prob = log_model.predict_proba(X_test)[:,1]\ny_pred = log_model.predict(X_test)\n\nfpr, tpr, threshold = roc_curve(y_test, y_pred_proba)\nroc_auc = roc_auc_score(y_test, y_pred_proba)\n\nplt.figure()\nplt.plot([0, 1], label='Baseline', linestyle='--')\nplt.plot(fpr, tpr, label='Regression')\nplt.title('Logistic Regression ROC AUC = %0.5f' % roc_auc)\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.legend(loc='lower right')\nplt.show()\n\nprint('accuracy_score:', accuracy_score(y_test, y_pred))\nprint('precision_score:', precision_score(y_test, y_pred))\nprint('recall_score:', recall_score(y_test, y_pred))\nprint('f1_score:', f1_score(y_test, y_pred))\nprint('roc_auc:', roc_auc_score(y_test, y_pred_proba))\nmodel_1 = Model(log_model, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(max_depth=9, class_weight= 'balanced', random_state=42)\nclf.fit(X_train, y_train)\n\ny_pred_proba = clf.predict_proba(X_test)[:, 1]\n\ny_pred = clf.predict(X_test)\nfpr, tpr, threshold = roc_curve(y_test, y_pred_proba)\nroc_auc = roc_auc_score(y_test, y_pred_proba)\n\nplt.figure()\nplt.plot([0, 1], label='Baseline', linestyle='--')\nplt.plot(fpr, tpr, label='Regression')\nplt.title('Logistic Regression ROC AUC = %0.5f' % roc_auc)\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.legend(loc='lower right')\nplt.show()\n\nprint('accuracy_score:', accuracy_score(y_test, y_pred))\nprint('precision_score:', precision_score(y_test, y_pred))\nprint('recall_score:', recall_score(y_test, y_pred))\nprint('f1_score:', f1_score(y_test, y_pred))\nprint('roc_auc:', roc_auc_score(y_test, y_pred_proba))\nmodel_2 = Model(clf, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier(max_depth = 6, class_weight= 'balanced')\ndtc.fit(X_train, y_train)\n\ny_pred_proba = dtc.predict_proba(X_test)[:, 1]\n\ny_pred = dtc.predict(X_test)\nfpr, tpr, threshold = roc_curve(y_test, y_pred_proba)\nroc_auc = roc_auc_score(y_test, y_pred_proba)\n\nplt.figure()\nplt.plot([0, 1], label='Baseline', linestyle='--')\nplt.plot(fpr, tpr, label='Regression')\nplt.title('Logistic Regression ROC AUC = %0.5f' % roc_auc)\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.legend(loc='lower right')\nplt.show()\n\nprint('accuracy_score:', accuracy_score(y_test, y_pred))\nprint('precision_score:', precision_score(y_test, y_pred))\nprint('recall_score:', recall_score(y_test, y_pred))\nprint('f1_score:', f1_score(y_test, y_pred))\nprint('roc_auc:', roc_auc_score(y_test, y_pred_proba))\nmodel_3 = Model(dtc, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier(n_estimators=500,learning_rate=0.1,max_depth=1)\ngb.fit(X_train, y_train)\n\ny_pred_proba = gb.predict_proba(X_test)[:, 1]\n\ny_pred = gb.predict(X_test)\nfpr, tpr, threshold = roc_curve(y_test, y_pred_proba)\nroc_auc = roc_auc_score(y_test, y_pred_proba)\n\nplt.figure()\nplt.plot([0, 1], label='Baseline', linestyle='--')\nplt.plot(fpr, tpr, label='Regression')\nplt.title('Logistic Regression ROC AUC = %0.5f' % roc_auc)\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.legend(loc='lower right')\nplt.show()\n\nprint('accuracy_score:', accuracy_score(y_test, y_pred))\nprint('precision_score:', precision_score(y_test, y_pred))\nprint('recall_score:', recall_score(y_test, y_pred))\nprint('f1_score:', f1_score(y_test, y_pred))\nprint('roc_auc:', roc_auc_score(y_test, y_pred_proba))\nmodel_4 = Model(gb, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Посмотрим на показатели моделей.\nmetrics = pd.concat([model_1.get_metrics(), model_2.get_metrics(),\n                     model_3.get_metrics(), model_4.get_metrics()],axis = 1)\nmetrics.columns = ['model_1', 'model_2', 'model_3', 'model_4']\nmetrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1.confusion_matrix()\nmodel_2.confusion_matrix()\nmodel_3.confusion_matrix()\nmodel_4.confusion_matrix()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Выводы:\n* Использовал (протестировал) несколько моделей.\n* Подбирал лучшие параметры для моделей, которые улучшали её. Но только для логистической регрессии, для деревьев и градиентного бустинга не успел.\n* С выбросами боролся логарифмированием.\n* бОльшая часть новых признаков ухудшала показатели модели. Также не успел провести работу с признаками, которые могли бы ухудшить модель.\n* Погуглил и нашёл информацию, что при логистической регрессии хорошо идут математические операции с признаками (при создании новых), что собстсвенно и использовал.\n* Хорошие данные важнее, чем хороший алгоритм!\n* В дальнейшем буду продолжать вести работу с данными и улучшать результат.","metadata":{}},{"cell_type":"code","source":"X_test = test_data.drop(['default'], axis=1)\ny_pred_prob = log_model.predict_proba(X_test)[:,1]\n\nsubmit = pd.DataFrame(test.client_id)\nsubmit['default']=y_pred_prob\nsubmit.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}